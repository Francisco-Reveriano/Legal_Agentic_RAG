{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T19:11:03.440172Z",
     "start_time": "2025-03-03T19:11:03.434724Z"
    }
   },
   "source": [
    "import time\n",
    "import asyncio\n",
    "from io import StringIO\n",
    "\n",
    "from onnxruntime.transformers.models.gpt2.gpt2_parity import score\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from src.data_processing import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API keys from environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "async def retriever(query: str, PINECONE_API_KEY: str, OPENAI_API_KEY:str, top_k: int = 20):\n",
    "    \"\"\"\n",
    "    Asynchronous retriever function that retrieves context relevant to a query by\n",
    "    utilizing embeddings generated by OpenAI and querying a Pinecone index. This\n",
    "    function combines metadata from matching results to form a consolidated\n",
    "    context output.\n",
    "\n",
    "    :param query: The input query for which relevant context must be retrieved\n",
    "    :param PINECONE_API_KEY: The API key for accessing the Pinecone service\n",
    "    :param OPENAI_API_KEY: The API key for accessing the OpenAI service\n",
    "    :param top_k: The maximum number of top results to retrieve from the index.\n",
    "                  Defaults to 10.\n",
    "    :return: A string containing the consolidated context generated by combining\n",
    "             unique metadata retrieved from the Pinecone index\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # Load Pinecone Index\n",
    "    pc = Pinecone(PINECONE_API_KEY)\n",
    "    index = pc.Index(\"legal-assistant-rag\")\n",
    "\n",
    "    # Load OpenAI Client\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    # Asynchronously create embedding\n",
    "    embedding_result = await loop.run_in_executor(\n",
    "        None,\n",
    "        lambda: client.embeddings.create(input=query, model=\"text-embedding-3-large\")\n",
    "    )\n",
    "    xq = embedding_result.data[0].embedding\n",
    "\n",
    "    # Asynchronously query the Pinecone index\n",
    "    res = await loop.run_in_executor(\n",
    "        None,\n",
    "        lambda: index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    )\n",
    "    res.matches = sorted(res.matches, key=lambda chunk: int(chunk['id']))\n",
    "\n",
    "    # Combine Chunks\n",
    "    chunk_list = []\n",
    "    for r in res.matches:\n",
    "        chunk_list.append(r[\"metadata\"][\"prechunk\"])\n",
    "        chunk_list.append(r[\"metadata\"][\"content\"])\n",
    "        chunk_list.append(r[\"metadata\"][\"postchunk\"])\n",
    "    unique_chunks = list(set(chunk_list))\n",
    "    question_context = \" \".join(unique_chunks)\n",
    "    return question_context"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T19:12:17.423294Z",
     "start_time": "2025-03-03T19:12:11.156514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are main requirements for Regulation K?\"\n",
    "top_k = 20\n",
    "\n",
    "# Load Pinecone Index\n",
    "pc = Pinecone(os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"legal-assistant-rag\")\n",
    "\n",
    "# Load OpenAI Client\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Asynchronously create embedding\n",
    "embedding_result = await loop.run_in_executor(\n",
    "    None,\n",
    "    lambda: client.embeddings.create(input=query, model=\"text-embedding-3-large\")\n",
    ")\n",
    "xq = embedding_result.data[0].embedding\n",
    "\n",
    "# Asynchronously query the Pinecone index\n",
    "res = await loop.run_in_executor(\n",
    "    None,\n",
    "    lambda: index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    ")\n",
    "res.matches = sorted(res.matches, key=lambda chunk: int(chunk['id']))"
   ],
   "id": "bba168d5d967362a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T19:14:22.096127Z",
     "start_time": "2025-03-03T19:14:22.085306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = []\n",
    "for r in res.matches:\n",
    "    scores.append(r[\"score\"])\n",
    "\n",
    "sns.histplot(scores)"
   ],
   "id": "649fbe97b766c523",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39mmatches:\n\u001B[1;32m      3\u001B[0m     scores\u001B[38;5;241m.\u001B[39mappend(r[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 5\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241m.\u001B[39mhistplot(scores)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
