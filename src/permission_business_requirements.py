import asyncio
import warnings
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from src.business_requirements import retriever

# Suppress any warnings for cleaner output
warnings.filterwarnings("ignore")



async def permissions_buss_req(original_question: str, business_requirement: str, detailed_business_requirement:str, OPENAI_API_KEY:str, PINECONE_API_KEY:str, model_name:str="gpt-4o",top_k:int=20) -> str:
    """
    Asynchronously generates a detailed permissions-based response derived from context,
    business requirements, and a user's original question using an AI model. This function
    uses a Language Model (LLM) and a retriever to gather and summarize information into
    a jurisdiction-appropriate list of actionable permissions.

    The function integrates multiple components:
    1. Constructs a structured prompt for determining permissions.
    2. Uses a language model to generate a response based on business context.
    3. Asynchronously fetches relevant documents based on input details using specified APIs.
    4. Processes and returns the response in Markdown format.

    :param original_question: A string representing the user's original query or question.
    :param business_requirement: A string capturing the verbatim business need or regulatory condition.
    :param detailed_business_requirement: A string elaborating on specific details related to the
        business requirement for clearer contextual understanding.
    :param OPENAI_API_KEY: A string containing the API key for accessing the OpenAI API.
    :param PINECONE_API_KEY: A string containing the API key for interacting with Pinecone services.
    :param model_name: The name of the language model to be used for the analysis. Defaults to "gpt-4o".
    :param top_k: An integer that specifies the number of top retrieved documents to consider.
        Defaults to 20.
    :return: A string containing the permissions list generated by the model, formatted as valid Markdown.
    """

    permission_requirement_detailed_prompt = '''
    #  **Detailed Prompt for Permissions*

    ## **Your Task**
    - Create a list of bullet points directly outlining what is permitted based on the provided **Context**, **Verbatim Business Requirement**, **Original Question**, and **Detailed Business Requirements* while ensuring clarity, accuracy, and legal precision
    - Provide output in correctly formatted Markdown
    
    ## **Key Characteristics of Permissions**
    - In a regulatory context, "permissions" refer to the specific actions, activities, or operations that a company is legally authorized to perform under applicable laws, regulations, and industry standards.

    ## **Input Sections**
    ### **Context**
    {context}

    ### **Verbatim Business Requirements**
    {business_requirement}

    ### **Original Question**
    {original_question}
    
    ### **Detailed Business Requirements**
    {detailed_business_requirement}

    '''

    # Create a chat prompt template using the detailed prompt.
    prompt = ChatPromptTemplate([
        ("system", permission_requirement_detailed_prompt),
    ])

    # Initialize the ChatOpenAI language model with a specific model name and temperature.
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, api_key=OPENAI_API_KEY)

    # Combine the prompt, the language model, and the output parser into a processing chain.
    rag_chain = prompt | llm | StrOutputParser()

    # Retrieve documents relevant to the query.
    # If `retriever` is a blocking function, run it in a separate thread.
    docs = await asyncio.to_thread(
        retriever,
        query=" ".join([original_question, business_requirement, detailed_business_requirement]),
        top_k=top_k,
        OPENAI_API_KEY=OPENAI_API_KEY,
        PINECONE_API_KEY=PINECONE_API_KEY,
    )

    # Asynchronously invoke the chain with the provided inputs.
    generation = await rag_chain.ainvoke({
        "context": docs,
        "original_question": original_question,
        "business_requirement": business_requirement,
        "detailed_business_requirement": detailed_business_requirement,
    })

    return generation
