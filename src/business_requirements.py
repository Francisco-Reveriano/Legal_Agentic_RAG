import time
import asyncio
from io import StringIO

from pinecone.grpc import PineconeGRPC as Pinecone
from IPython.display import Markdown, display
from openai import OpenAI
from src.data_processing import *
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

async def retriever(query: str, PINECONE_API_KEY: str, OPENAI_API_KEY:str, top_k: int = 20):
    """
    Asynchronous retriever function that retrieves context relevant to a query by
    utilizing embeddings generated by OpenAI and querying a Pinecone index. This
    function combines metadata from matching results to form a consolidated
    context output.

    :param query: The input query for which relevant context must be retrieved
    :param PINECONE_API_KEY: The API key for accessing the Pinecone service
    :param OPENAI_API_KEY: The API key for accessing the OpenAI service
    :param top_k: The maximum number of top results to retrieve from the index.
                  Defaults to 10.
    :return: A string containing the consolidated context generated by combining
             unique metadata retrieved from the Pinecone index
    :rtype: str
    """

    # Load Pinecone Index
    pc = Pinecone(PINECONE_API_KEY)
    index = pc.Index("legal-assistant-rag")

    # Load OpenAI Client
    client = OpenAI(api_key=OPENAI_API_KEY)

    loop = asyncio.get_event_loop()

    # Asynchronously create embedding
    embedding_result = await loop.run_in_executor(
        None,
        lambda: client.embeddings.create(input=query, model="text-embedding-3-large")
    )
    xq = embedding_result.data[0].embedding

    # Asynchronously query the Pinecone index
    res = await loop.run_in_executor(
        None,
        lambda: index.query(vector=xq, top_k=top_k, include_metadata=True)
    )
    res.matches = sorted(res.matches, key=lambda chunk: int(chunk['id']))

    # Combine Chunks
    chunk_list = []
    for r in res.matches:
        chunk_list.append(r["metadata"]["prechunk"])
        chunk_list.append(r["metadata"]["content"])
        chunk_list.append(r["metadata"]["postchunk"])
    unique_chunks = list(set(chunk_list))
    question_context = " ".join(unique_chunks)
    return question_context


def verbatim_business_requirements(query:str, OPENAI_API_KEY:str, PINECONE_API_KEY:str, top_k:int=40):
    """
    Extract and format regulatory requirements as structured business rules in a CSV-compatible
    format. The function utilizes Pinecone for indexing and querying context data and OpenAI
    for generating embeddings and processing the regulatory text. Verbatim legal requirements
    from the provided text, complying with specified formatting and guidelines, are returned.

    :param query: Regulatory text input for extracting business requirements.
    :type query: str
    :param OPENAI_API_KEY: API key for accessing OpenAI services.
    :type OPENAI_API_KEY: str
    :param PINECONE_API_KEY: API key for accessing Pinecone services.
    :type PINECONE_API_KEY: str
    :param top_k: Number of top similar context chunks to retrieve from Pinecone index.
        Defaults to 40.
    :type top_k: int, optional
    :return: CSV string containing extracted business requirements aligned with the
        specified output format.
    :rtype: str
    :raises ValueError: If the combined context token count exceeds the 100,000 token limit.
    """
    # Load Pinecone Index
    pc = Pinecone(PINECONE_API_KEY)
    index = pc.Index("legal-assistant-rag")

    # Load OpenAI Client
    client = OpenAI(api_key=OPENAI_API_KEY)
    xq = client.embeddings.create(input=query, model="text-embedding-3-large").data[0].embedding
    res = index.query(vector=xq, top_k=top_k, include_metadata=True)
    res.matches = sorted(res.matches, key=lambda chunk: int(chunk['id']))
    # Combine Chunks
    chunk_list = []
    for r in res.matches:
        chunk_list.append(r["metadata"]["prechunk"])
        chunk_list.append(r["metadata"]["content"])
        chunk_list.append(r["metadata"]["postchunk"])
    unique_chunks = list(set(chunk_list))
    question_context = " ".join(unique_chunks)
    chunk_token_count = count_tokens_gpt4(question_context)

    # Error handling: raise an error if the token count exceeds 100k tokens
    if chunk_token_count > 100000:
        raise ValueError(
            f"Combined context token count ({chunk_token_count}) exceeds the 100k limit. "
            "Please provide a shorter input or adjust the context."
        )


    developer_prompt = '''
    # **Regulatory Requirement Extraction & Formatting**  

    ## **Instructions**  
    Your task is to extract and format **clear, explicit legal requirements** from the given regulatory text. These requirements must be presented in a structured format for easy reference and compliance tracking.  
    
    ---  
    
    ## **Regulatory Text**  
    {}

    ## **Output Format**  
    Provide the extracted requirements as a **CSV-parsable table** using the following structure:  
    
    | Business Requirement |  
    |----------------------|  
    | [Extracted requirement 1] |  
    | [Extracted requirement 2] |  
    | … |  
    
    - Each row must contain **one standalone regulatory requirement**.  
    - Maintain the **exact wording** from the regulation—**do not paraphrase, summarize, or interpret**.  
    
    ---  
    
    ## **Extraction Guidelines**  
    1. **Explicit Legal Requirements Only**: Extract only **clearly defined legal obligations**—statements that mandate, prohibit, or require specific actions.  
    2. **No Contextual or Advisory Text**: Exclude general guidance, definitions, explanations, or recommendations.  
    3. **Verbatim Extraction**: Use the **exact wording** from the regulatory text. **Do not alter phrasing**.  
    4. **Structural Integrity**: Ensure the extracted requirements are **properly formatted** and **CSV-compatible**.  
    5. **Complete Coverage**: Review the entire regulatory text to extract **all** relevant requirements.  
    6. **Delimiter & Formatting**:  
       - Use `"^^"` as the delimiter for the CSV output.  
       - Ensure **each row is properly formatted** and compatible with CSV parsing.  
    7. **Final Validation**: Before outputting, **verify** that:  
       - The delimiter `"^^"` is consistently applied.  
       - Each requirement is properly structured in its own row.  
    
    ---  
    
    ## **Output Format Specification**  
    - Output must be a **CSV string** formatted with `"^^"` as the delimiter.  
    - Ensure **no additional text** is included outside of the CSV table.  
    - Verify **correct formatting** before returning the output.  
    
    ---  
    '''.format(question_context)

    completion = client.chat.completions.create(
        model="o3-mini",
        reasoning_effort="medium",
        messages=[
            {"role": "developer", "content": developer_prompt},
            {"role": "user", "content": query}
        ]
    )

    return completion.choices[0].message.content

def convert_str_to_df(data_str:str, OPENAI_API_KEY:str, retries=5):
    """
    Converts a string representation of tabular data into a Pandas DataFrame. The input
    string is expected to adhere to a delimiter-separated format. The method first
    attempts direct parsing using Pandas and, upon failure after a specified number of
    retries, seeks assistance from OpenAI's GPT-based API to generate a corrected CSV
    format. A final fallback mechanism attempts to parse the input while skipping
    invalid lines.

    :param data_str: Input data as a string formatted with delimiter-separated values.
    :param OPENAI_API_KEY: API key required to access OpenAI's GPT for data correction.
    :param retries: Number of attempts to parse the input string using Pandas before
        resorting to GPT-based correction.
    :return: A Pandas DataFrame containing the parsed data.
    :rtype: pandas.DataFrame
    :raises Exception: Raised if all parsing attempts, including the final fallback,
        fail to produce a valid DataFrame.
    """
    last_exception = None
    for attempt in range(1, retries + 1):
        try:
            # Use the python engine for better error tolerance
            df = pd.read_csv(StringIO(data_str), sep="^^", engine='python')
            return df
        except pd.errors.ParserError as pe:
            print(f"Attempt {attempt}: ParserError encountered: {pe}")
            # On the final attempt, try GPT conversion
            if attempt == retries:
                try:
                    client = OpenAI(api_key=OPENAI_API_KEY)
                    completion = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "developer", "content": markdown_to_csv_prompt},
                            {"role": "user", "content": data_str}
                        ]
                    )
                    new_csv = completion.choices[0].message.content
                    df = pd.read_csv(StringIO(new_csv), sep="^^", engine='python', on_bad_lines='skip')
                    return df
                except Exception as gpt_e:
                    last_exception = gpt_e
                    print(f"GPT conversion attempt failed: {gpt_e}")
                    break
        except Exception as e:
            last_exception = e
            print(f"Attempt {attempt}: Unexpected error encountered: {e}")

    # Final attempt: skip bad lines if previous attempts fail
    try:
        print("Final attempt: trying to read by skipping bad lines.")
        df = pd.read_csv(StringIO(data_str), sep="^^", on_bad_lines='skip', engine='python')
        print("DataFrame created by skipping problematic lines.")
        return df
    except Exception as final_e:
        print("Final attempt failed.")
        raise Exception(f"Failed to convert string to DataFrame after {retries} retries and skipping bad lines.") from final_e
